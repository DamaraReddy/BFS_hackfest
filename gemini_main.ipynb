{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL BUILD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\2272655\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import PyPDF2\n",
    "import time\n",
    "from azure.storage.blob import ContainerClient\n",
    "# from openai import AzureOpenAI\n",
    "\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up Azure Blob Storage connection\n",
    "connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "container_name = os.getenv(\"CONTAINER_NAME\")  # Replace with your container name\n",
    "container_client = ContainerClient.from_connection_string(\n",
    "    conn_str=connection_string,\n",
    "    container_name=container_name\n",
    ")\n",
    "\n",
    "# Function to extract text from a PDF file stream\n",
    "def extract_text_from_pdf(file_stream):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        pdf_reader = PyPDF2.PdfReader(file_stream)\n",
    "        for page in pdf_reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "    return text\n",
    "\n",
    "# Function to truncate text to fit within the token limit\n",
    "# def truncate_text(input_text, max_tokens):\n",
    "#     tokens = input_text.split()  # Naive tokenization by splitting on spaces\n",
    "#     if len(tokens) > max_tokens:\n",
    "#         truncated_text = ' '.join(tokens[:max_tokens])\n",
    "#         return truncated_text\n",
    "#     return input_text\n",
    "\n",
    "\n",
    "def get_json(file_stream, k):\n",
    "\n",
    "    genai.configure(api_key=os.getenv(f\"GEMINI_API_KEY_{k}\"))\n",
    "    \n",
    "    \n",
    "    # Extract text from PDF\n",
    "    input_context = extract_text_from_pdf(file_stream)\n",
    "\n",
    "    instructions = \"\"\"\n",
    "     As a Financial Analyst, you will leverage your expertise to generate tailored Financial Analysis \n",
    "     Reports that cater to the specific requirements of clients. \n",
    "    Key Objectives:\n",
    "\n",
    "    Extract and Present: Provide detailed figures for each metric listed above, ensuring accurate and up-to-date data.\n",
    "    Categorize Clearly: Organize the metrics under their respective categories as outlined.\n",
    "    Ensure Completeness: Verify that all relevant metrics are included and presented with sufficient context for accurate interpretation.\n",
    "    Highlight Capital Expenditure: Ensure that capital expenditure-related metrics are prominently detailed and clearly separated from other categories.\n",
    "    \"\"\"\n",
    "    model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\", system_instruction=instructions)\n",
    "    client = model.start_chat()\n",
    "    \n",
    "    user_content = f\"\"\"\n",
    "    To extract and organize financial metrics from the report, including those related to capital expenditure, profitability, liquidity, solvency, cash flow, and other key indicators. The metrics should be categorized and detailed according to their relevance to different aspects of financial analysis. \n",
    "\n",
    "    Operating Activities \"Change in Working Capital\",\n",
    "            \"Net Cash from Operating Activities\"\n",
    "    Investing Activities has \"Acquisition of Fixed Assets & Intangibles\",\n",
    "            \"Net Cash from Investing Activities\"\n",
    "    Financing Activities includes \"Dividends Paid\",\n",
    "            \"Cash from (Repayment of) Debt\",\n",
    "            \"Net Cash from Financing Activities\",\n",
    "    Net Change includes  Net Change in Cash\n",
    "    Metadata includes \"Report Date\", \"Publish Date\", \"Source\"\n",
    "    Profitability Metrics like \"EBITDA\",\n",
    "            \"Gross Profit Margin\",\n",
    "            \"Operating Margin\",\n",
    "            \"Net Profit Margin\",\n",
    "            \"Return on Equity\",\n",
    "            \"Return on Assets\",\n",
    "            \"Return On Invested Capital\",\n",
    "    Liquidity Metrics include Current Ratio\n",
    "    Solvency Metrics like  \"Total Debt\", \"Liabilities to Equity Ratio\", \"Debt Ratio\",\n",
    "    Cash Flow Metrics like \"Free Cash Flow\", \"Free Cash Flow to Net Income\", \"Cash Return On Invested Capital\",\n",
    "    Other Important Metrics like \"Piotroski F-Score\", \"Net Debt / EBITDA\", \"Dividend Payout Ratio\n",
    "    Capital Expenditure \n",
    "    ```Document\n",
    "    {input_context}\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    # JSON output structure request\n",
    "    final_content = \"\"\"\n",
    "    Please provide the company name, year and quarter, capex value in billions of US dollars and no need of use $ symbol \n",
    "    in JSON format.\n",
    "    Use the following JSON structure:\n",
    "    ```json\n",
    "    {\n",
    "        \"company\": \"\",\n",
    "        \"year\": \"\",\n",
    "        \"quarter\": \"\",\n",
    "        \"capex\": \"\"    \n",
    "    }\n",
    "    ```\n",
    "\n",
    "    Example:\n",
    "    ```json\n",
    "    {\n",
    "        \"company\": \"abc\",\n",
    "        \"year\": \"2023\",\n",
    "        \"quarter\": \"3\",\n",
    "        \"capex\": \"4.9\"    \n",
    "    }\n",
    "    ```\n",
    "\n",
    "    **Note: Only JSON Data is required, no other text is required.**\n",
    "    \"\"\"\n",
    "\n",
    "    # Steps to find the capital expenditure value\n",
    "    # how_content = \"\"\"\n",
    "    # Please provide the steps how you found or calculated the capital expenditure value.\n",
    "    # \"\"\"\n",
    "\n",
    "    print(f\"*************API - {k} - IN USE***************\")\n",
    "    retries = 1\n",
    "    while retries <= 3:\n",
    "        try:\n",
    "            response = client.send_message(user_content)\n",
    "            # print(\"1st response saved...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            if retries > 3:\n",
    "                print(\"!!!!!Couldn't Resolve!!!!!!\")\n",
    "                return None\n",
    "            print(f\"Error Occoured for API {k}: {e}\\n Retrying...{retries}\")\n",
    "            time.sleep(15)\n",
    "\n",
    "    retries = 1\n",
    "    while retries <= 3:\n",
    "        try:\n",
    "            # print(\"getting json...\")\n",
    "            response_json = client.send_message(final_content)\n",
    "            print(f\"done - {response_json.text}\")\n",
    "            # how_respose = client.send_message(how_content)\n",
    "            return response_json.text # how_respose.text\n",
    "        except Exception as e:\n",
    "            retries += 1\n",
    "            if retries > 3:\n",
    "                print(\"!!!!!Couldn't Resolve!!!!!!\")\n",
    "                return None\n",
    "            print(f\"Error Occoured for API {k}: {e}\\n Retrying...{retries}\")\n",
    "            time.sleep(15)\n",
    "\n",
    "def process_blob(blob, k=1, retries=2,):\n",
    "    attempt = 0\n",
    "    while attempt < retries:\n",
    "        try:\n",
    "            stream = io.BytesIO()\n",
    "            container_client.download_blob(blob).readinto(stream)\n",
    "            op = get_json(stream, k)\n",
    "            if op: #and how:\n",
    "                op = json.loads(op[op.index(\"{\"):len(op)-op[::-1].index(\"}\")])\n",
    "                if op.get(\"capex\"):  # Check if capex is not empty\n",
    "                    return op #, how\n",
    "                else:\n",
    "                    print(f\"Capex is empty. Retrying for blob {blob.name}...\")\n",
    "            attempt += 1\n",
    "            time.sleep(5)  # Wait before retrying\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing blob {blob.name}: {e}\")\n",
    "            return None#, None\n",
    "    print(f\"Max retries reached for blob {blob.name}.\")\n",
    "    return None#, None\n",
    "\n",
    "# function to process blobs in parallel\n",
    "def get_capex_info(companies):\n",
    "    final_json = []\n",
    "    blob_list = list(container_client.list_blobs())[:]  # Limit to the first 20 blobs\n",
    "    # companies = companies\n",
    "    # [\"Alphabet\", \"Amazon\", \"Berkshire Hathaway\", \n",
    "    #              \"Cardinal Health\", \"Chevron\", \"Fannie Mae\",\n",
    "    #              \"Goldman Sachs Group\", \"Valero Energy\", \n",
    "    #              \"Morgan Stanley\",\"Tesla\"]\n",
    "    blob_list = [blob for blob in blob_list if blob.name.strip().split('/')[0] in companies]\n",
    "    print(f\"NUMBER OF FILES: {len(blob_list)}\")\n",
    "    completed = 0\n",
    "    # Using ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        future_to_blob = {executor.submit(process_blob, blob, k%7): (blob, k) for k, blob in enumerate(blob_list)}\n",
    "        # Iterate over completed futures\n",
    "        for future in as_completed(future_to_blob):\n",
    "            blob = future_to_blob[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                if result:\n",
    "                    op = result\n",
    "                    if op:  # Ensure op is not None\n",
    "                        completed += 1\n",
    "                        print(f\"{completed} files completed.\")\n",
    "                        print(op, \"\\n\\n\") #, how)\n",
    "                        final_json.append(op)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing result for blob {blob.name}: {e}\")\n",
    "\n",
    "    # Ensure final_json contains valid data\n",
    "    final_json = [item for item in final_json if isinstance(item, dict)]\n",
    "    # Optional: Save final JSON to a file\n",
    "    # output_file = \"financial_analysis_results_1201.json\"\n",
    "    # with open(output_file, \"w\") as outfile:\n",
    "    #     json.dump(final_json, outfile, indent=4)\n",
    "    # final_json = json.loads(final_json)\n",
    "    # Convert final_json to DataFrame and save to CSV\n",
    "    if final_json:  # Check if final_json is not empty\n",
    "        df = pd.DataFrame(final_json)\n",
    "        # output_file = \"financial_analysis_results10.csv\"\n",
    "        # df.to_csv(output_file, index=False)\n",
    "        print(f\"Total {completed} files completed.\")\n",
    "        print(f\"Processing completed. Results saved to df.\")\n",
    "        return df, final_json\n",
    "        # print(f\"Processing completed. Results saved to {output_file}.\")\n",
    "    else:\n",
    "        # print(\"No valid data to save to CSV.\")\n",
    "        print(\"No valid data to save to df.\")\n",
    "        return None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "# Read the original CSV file\n",
    "# input_file = \"all_1.csv\"\n",
    "# df = pd.read_csv(input_file)\n",
    "\n",
    "def clean_df(df):\n",
    "    df['company'] = df['company'].replace({\n",
    "        'Amazon': 'Amazon',\n",
    "        'Amazon.com, Inc.': 'Amazon',\n",
    "        'Alphabet Inc':'Alphabet',\n",
    "        'Amazon.com Inc.': 'Amazon',\n",
    "        'Goldman Sachs' : 'Goldman Sachs Group',\n",
    "        'The Goldman Sachs Group, Inc.': 'Goldman Sachs Group',\n",
    "        'Goldman Sachs Group':'Goldman Sachs Group',\n",
    "        'Tesla' : 'Tesla',\n",
    "        'Tesla, Inc.':'Tesla',\n",
    "        'Tesla Motors, Inc.': 'Tesla',\n",
    "        'Tesla Motors':'Tesla',\n",
    "        'Valero Energy Corporation': 'Valero Energy',\n",
    "        'Cardinal Health':'Cardinal Health',\n",
    "        'BERKSHIRE HATHAWAY INC.': 'Berkshire Hathaway',\n",
    "        'Berkshire Hathaway Inc' : 'Berkshire Hathaway',\n",
    "        '\"Cardinal Health, Inc.\"':'Cardinal Health',\n",
    "        'Chevron Corporation': 'Chevron',\n",
    "\n",
    "    })\n",
    "\n",
    "    df['capex'] = df['capex'].replace({ 'Not provided in the report':'-', 'nan':'-' ,'Not provided': '-','Not Provided': '-','Not mentioned in the report':'-','X':'-', 'Not available': '-',  'Not specified': '-', 'n/a': '-', 'nan': '-','X.XX': '-','Not provided in the Report':'-','Not Provided in the Report':'-','Not available in the provided document':'-', 'Not disclosed':'-'})\n",
    "    df['company'] = df['company'].str.replace('\"', ' ', regex=False)\n",
    "    df['year'] = df['year'].replace({'FY16' :'2016', 'FY21':'2021', 'FY20':'2020', 'FY19':'2019', 'FY23':'2023', 'FY24':'2024', 'FY2024':'2024'})\n",
    "    df['quarter'] = df['quarter'].replace({'First Quarter': 'Q1', '2': 'Q2', 'Second Quarter': 'Q2', 'Third Quarter': 'Q3',  'Fourth Quarter' : 'Q4','Fourth':'Q4','fourth':'Q4', 'Second':'Q2','4': 'Q4','First':'Q1', '4Q':'Q4','4':'Q4', 'Third':'Q3', '1Q':'Q1', 1:'Q1', 2:'Q2', 3:'Q3', 4:'Q4','1':'Q1', '2':'Q2', '3':'Q3', '4':'Q4'})\n",
    "    # Save the rearranged DataFrame to a new CSV file\n",
    "    # output_file = \"cleaned_financial_analysis_results1.csv\"\n",
    "    # df.to_csv(output_file, index=False)\n",
    "    # output_file = df\n",
    "    print(f\"cleaned data saved\")# to {output_file}.\")\n",
    "    return df\n",
    "\n",
    "## Rearranged input capex with quarter_year with company name \n",
    "\n",
    "# Read the original CSV file\n",
    "# input_file = \"cleaned_financial_analysis_results1.csv\"\n",
    "# df = pd.read_csv(input_file)\n",
    "# input_file = output_file\n",
    "# df = input_file\n",
    "def rearrange_df(df):\n",
    "    # Convert 'quarter' and 'year' columns into a single 'Quarter-Year' column with an underscore\n",
    "    df['Quarter-Year'] = df['quarter'] + \"_\" + df['year'].astype(str)\n",
    "\n",
    "    # Pivot the DataFrame\n",
    "    pivot_df = df.pivot_table(index='company', columns='Quarter-Year', values='capex', aggfunc='first')\n",
    "\n",
    "    # Reset the index to make 'company' a column instead of an index\n",
    "    pivot_df.reset_index(inplace=True)\n",
    "\n",
    "    # Define a function to sort columns in the desired order\n",
    "    def sort_columns(df):\n",
    "        # Extract the current columns\n",
    "        columns = df.columns.tolist()\n",
    "\n",
    "        # Extract the company column and the Quarter-Year columns\n",
    "        company_col = columns[0]\n",
    "        quarter_cols = columns[1:]\n",
    "\n",
    "        # Generate sorted columns list: Start with the most recent quarters\n",
    "        sorted_quarters = sorted(quarter_cols, key=lambda x: (x.split('_')[1], x.split('_')[0]), reverse=True)\n",
    "\n",
    "        # Combine sorted columns with the company column\n",
    "        sorted_columns = [company_col] + sorted_quarters\n",
    "        return sorted_columns\n",
    "\n",
    "    # Reorder the columns\n",
    "    sorted_columns = sort_columns(pivot_df)\n",
    "    pivot_df = pivot_df[sorted_columns]\n",
    "    pivot_df['company'].unique()\n",
    "    pivot_df['company'] = pivot_df['company'].str.replace('\"', ' ', regex=False)\n",
    "    # # Save the rearranged DataFrame to a new CSV file\n",
    "    # output_file = pivot_df\n",
    "    # \"rearranged_financial_analysis_results1.csv\"\n",
    "    # pivot_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Rearranged data saved\")# to {output_file}.\")\n",
    "    return pivot_df\n",
    "\n",
    "def final_process(df):\n",
    "    # Define a function to compare capex values between consecutive columns\n",
    "    def compare_columns(row):\n",
    "        comparisons = []\n",
    "        for i in range(1, len(row) - 1):  # Skip the 'company' column\n",
    "            current_value = row[i]\n",
    "            next_value = row[i + 1]\n",
    "\n",
    "            if pd.isna(current_value) or pd.isna(next_value):\n",
    "                comparisons.append(\"DNA\")#(\"Data not available\")\n",
    "            else:\n",
    "                try:\n",
    "                    current_value = float(current_value)\n",
    "                    next_value = float(next_value)\n",
    "                    if next_value < current_value:\n",
    "                        comparisons.append(\"Increase\")\n",
    "                    elif next_value > current_value:\n",
    "                        comparisons.append(\"Decrease\")\n",
    "                    else:\n",
    "                        comparisons.append(\"Unchanged\")\n",
    "                except ValueError:\n",
    "                    comparisons.append(\"DNA\")#(\"Data not available\")\n",
    "\n",
    "        return comparisons\n",
    "\n",
    "    # Apply the comparison function to each row\n",
    "    comparison_results = df.apply(lambda row: compare_columns(row), axis=1)\n",
    "\n",
    "    # Replace the original values in the DataFrame with the comparison results\n",
    "    for i, col in enumerate(df.columns[1:-1]):  # Exclude 'company' and the last quarter\n",
    "        df[col] = comparison_results.apply(lambda x: x[i])\n",
    "\n",
    "    # # Save the comparison results to a new CSV file\n",
    "    # output_file = \"capex_comparison_results.csv\"\n",
    "    # df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Capex comparison results saved\") #to {output_file}.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "company_df = []\n",
    "raw_company_df = []\n",
    "raw_json = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########processing--Alphabet##########\n",
      "NUMBER OF FILES: 14\n",
      "*************API - 0 - IN USE***************\n",
      "*************API - 1 - IN USE***************\n",
      "*************API - 2 - IN USE***************\n",
      "Error Occoured for API 0: 429 Quota exceeded for quota metric 'Generate Content API requests per minute' and limit 'GenerateContent request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:724959239723'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/generate_content_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-west4\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"GenerateContentRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"0\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/724959239723\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quota#requesting_higher_quota\"\n",
      "}\n",
      "]\n",
      " Retrying...2\n",
      "*************API - 3 - IN USE***************\n",
      "Error Occoured for API 1: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "]\n",
      " Retrying...2\n",
      "Error Occoured for API 2: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "]\n",
      " Retrying...2\n",
      "Error Occoured for API 3: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "]\n",
      " Retrying...2\n",
      "Error Occoured for API 0: 429 Quota exceeded for quota metric 'Generate Content API requests per minute' and limit 'GenerateContent request limit per minute for a region' of service 'generativelanguage.googleapis.com' for consumer 'project_number:724959239723'. [reason: \"RATE_LIMIT_EXCEEDED\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_metric\"\n",
      "  value: \"generativelanguage.googleapis.com/generate_content_requests\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_location\"\n",
      "  value: \"us-west4\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit\"\n",
      "  value: \"GenerateContentRequestsPerMinutePerProjectPerRegion\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"quota_limit_value\"\n",
      "  value: \"0\"\n",
      "}\n",
      "metadata {\n",
      "  key: \"consumer\"\n",
      "  value: \"projects/724959239723\"\n",
      "}\n",
      ", links {\n",
      "  description: \"Request a higher quota limit.\"\n",
      "  url: \"https://cloud.google.com/docs/quota#requesting_higher_quota\"\n",
      "}\n",
      "]\n",
      " Retrying...3\n",
      "Error Occoured for API 1: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "]\n",
      " Retrying...3\n",
      "Error Occoured for API 2: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "]\n",
      " Retrying...3\n",
      "Error Occoured for API 3: 400 API key expired. Please renew the API key. [reason: \"API_KEY_INVALID\"\n",
      "domain: \"googleapis.com\"\n",
      "metadata {\n",
      "  key: \"service\"\n",
      "  value: \"generativelanguage.googleapis.com\"\n",
      "}\n",
      "]\n",
      " Retrying...3\n"
     ]
    }
   ],
   "source": [
    "# all_companies = [\"Alphabet\", \"Amazon\", \"Berkshire Hathaway\", \n",
    "#              \"Cardinal Health\", \"Chevron\", \"Fannie Mae\",\n",
    "#              \"Goldman Sachs Group\", \"Valero Energy\", \n",
    "#              \"Morgan Stanley\",\"Tesla\"]\n",
    "\n",
    "all_companies = [\n",
    "\"Alphabet\",\n",
    "\"Amazon\",\n",
    "\"Berkshire Hathaway\",\n",
    "\"Cardinal Health\",\n",
    "\"Centene\",\n",
    "\"Chevron\",\n",
    "\"Comcast\",\n",
    "\"ExxonMobil\",\n",
    "\"Tesla\",\n",
    "\"UnitedHealth Group\",\n",
    "\"Valero Energy\",\n",
    "\"Walgreens Boots Alliance\",\n",
    "\"Walmart\"\n",
    "]\n",
    "for company in all_companies[:1]:\n",
    "    print(f\"{'#'*10}processing--{company}{'#'*10}\")\n",
    "    csv_df, json_data = get_capex_info([company])\n",
    "    raw_json.append(json_data)\n",
    "    cleaned_df = clean_df(csv_df)\n",
    "    rearranged_df = rearrange_df(cleaned_df)\n",
    "    rearranged_df.to_csv(f\"{company}.csv\")\n",
    "    raw_company_df.append(rearranged_df.copy(deep=True))\n",
    "    final_df = final_process(rearranged_df)\n",
    "    final_df.to_csv(f\"final_{company}.csv\")\n",
    "    company_df.append(final_df.copy(deep=True))\n",
    "    print(f\"{'$'*10}processed--{company}{'$'*10}\")\n",
    "# company = \"Alphabet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
